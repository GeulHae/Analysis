{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKZJSg82cvUBkEafFTphlS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeulHae/Analysis/blob/dev_dataAnalysis/TRL_%2B_GPT2%EA%B0%80_%ED%8F%AC%ED%95%A8%EB%90%9C_RLHF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRL + GPT2가 포함된 RLHF"
      ],
      "metadata": {
        "id": "8PvMeUJLdUZk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eq28P55AdO0M"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/lvwerra/trl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead, create_reference_model\n",
        "from trl.core import respond_to_batch\n",
        "\n",
        "config = {\n",
        "    \"model_name\": \"gpt2\",\n",
        "    \"steps\": 20000,\n",
        "    \"batch_size\": 256,\n",
        "    \"forward_batch_size\": 16,\n",
        "    \"ppo_epochs\": 4,\n",
        "#     \"txt_in_min_len\": 2,\n",
        "#     \"txt_in_max_len\": 8,\n",
        "#     \"txt_out_min_len\": 4,\n",
        "#     \"txt_out_max_len\": 16,\n",
        "#    \"lr\": 1.41e-5,\n",
        "    \"init_kl_coef\":0.2,\n",
        "    \"target\": 6,\n",
        "    \"horizon\":10000,\n",
        "    \"gamma\":1,\n",
        "    \"lam\":0.95,\n",
        "    \"cliprange\": .2,\n",
        "    \"cliprange_value\":.2,\n",
        "    \"vf_coef\":.1,\n",
        "}\n",
        "# initialize trainer\n",
        "ppo_config = PPOConfig(**config)"
      ],
      "metadata": {
        "id": "wMxxZPeAdYqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# get models\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(config['model_name'])\n",
        "model_ref = create_reference_model(model)\n",
        "\n",
        "model.to(device)\n",
        "model_ref.to(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(config['model_name'])"
      ],
      "metadata": {
        "id": "cSDQTmxWdbLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# encode a query\n",
        "query_txt = [\"What's your job?\"]\n",
        "query_tensor = tokenizer(query_txt, return_tensors=\"pt\", padding=True)['input_ids']\n",
        "query_tensor = query_tensor.to(device)\n",
        "query_list = [query_tensor[0]]*256\n",
        "\n",
        "# get model response\n",
        "response_tensor  = respond_to_batch(model_ref, query_tensor)"
      ],
      "metadata": {
        "id": "0r1DagXtdcyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy response\n",
        "response_txt = [\"I'm the mailman\"]\n",
        "response_tensor = tokenizer(response_txt, return_tensors=\"pt\", padding=True)['input_ids']\n",
        "response_tensor = response_tensor.to(device)\n",
        "response_list = [response_tensor[0]]*256"
      ],
      "metadata": {
        "id": "AEQOEuHDdcxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a ppo trainer\n",
        "ppo_trainer = PPOTrainer(ppo_config, model, model_ref, tokenizer)\n",
        "device = ppo_trainer.accelerator.device\n",
        "\n",
        "# define a reward for response\n",
        "# (this could be any reward such as human feedback or output from another model)\n",
        "reward_list = [torch.tensor(10)] * 256\n",
        "\n",
        "# train model for one step with ppo\n",
        "train_stats = ppo_trainer.step(query_list, response_list, reward_list)"
      ],
      "metadata": {
        "id": "4G5gSpFOdcvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = tokenizer('What is your job?', return_tensors='pt').to(device).input_ids\n",
        "\n",
        "response_tensor  = respond_to_batch(model, test_input)\n",
        "tokenizer.batch_decode(response_tensor)"
      ],
      "metadata": {
        "id": "k6zTOUk6dcsL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}